[default]

##
##  Graphite incoming and a relay backend
##
##  This config listens on :2003 (`graphite-proxy`),
##  maintains another backend-only server set (`graphite-gg-relay`) pointing to :2013
##  this is ideal for acting like `carbon-relay.py + carbon-aggregator.py`
##
##  see `graphite-relay-preg.toml` for the relay setup
##


## PID file to check if things is running already
pid_file="/opt/cadent/graphite.pid"

## performance tuning suggest that n - 2 is best to avoid over creation
## of File Descriptors for the many sockets we need (and thus cause more OS
## overhead as time goes on)

num_procs=4

stats_tick=false


## cache keys/server pairs in an LRU mem cache for speed
cache_items=100000

## Health Checking options
### check every "X" seconds
heartbeat_time_delay=60

### Timeout for failed connections (in Seconds)
heartbeat_time_timeout=1

## if a server has failed "X" times it's out of the loop
failed_heartbeat_count=3

## If we detect a downed node,
## `remove_node` -- we can REMOVE the node from the hashpool
## `ignore` -- stop checking the node
server_down_policy="ignore"

## Turn on CPU/Mem profiling
##  there will be a http server set up to yank pprof data on :6060
cpu_profile=false
block_profile=false # this is very expensive

## fire out some internal to statsd if desired
statsd_server="localhost:8125"
statsd_prefix="consthash-graphite"
statsd_interval=1  # send to statd every second (buffered)
# global statsd Sample Rates

## It's HIGHLY recommended you at least put a statsd_timer_sample_rate as we measure
## rates of very fast functions, and it can take _alot_ of CPU cycles just to do that
## unless your server not under much stress 1% is a good number
## `statsd_sample_rate` is for counters
## `statsd_timer_sample_rate` is for timers
statsd_timer_sample_rate=0.01
statsd_sample_rate=0.1


## outgoing connections are pooled per outgoign server
## this sets the number of pool connections
## 10 seems to be a "good" number for us
max_pool_connections=20

## if using the `bufferedpool` this is the size in bytes to use for the buffer
## (defaults to 512)
pool_buffersize=1024

## If using a UDP input, you may wish to set the buffer size for clients
## default is 1Mb
read_buffer_size=1048576
max_buffer_size=104857600

## number of workers to chew on the sending queue
## adjust this based on the input load (1-5)
## one does a pretty good job so start there
workers=1

## there will be an internal health http server set up as well
## and will respond to '/ops/status' -> "GET OK"
## '/stats' --> blob of json data (adding ?jsonp=xxx will return jsonp data)
## '/' --> a little file that charts (poorly) the above stats
internal_health_server_listen="0.0.0.0:6061"

## stats are "rendered" every 5 seconds, so this will keep last
## `internal_health_server_points` in RAM (careful with how many you store)
## (use the statsd emitter to store longer term stuff)
internal_health_server_points=500


[graphite-proxy]

### Server sections .. note all the above is overridable in each section
### (except the `statsd_*` and `internal_health_server_listen`)
### NOTE: `hasher_replicas` will NOT use the defaults, be explict there

## what port/scheme we are listening on for incoming data
listen="tcp://0.0.0.0:2003"

msg_type="graphite"

# graphite style
hasher_algo="md5"
hasher_elter="graphite"
hasher_vnodes=1

## need higher timeouts for graphite
write_timeout=5000
runner_timeout=5000
pool_buffersize=16384

#TCP buffers should be smaller as there are Many TCP conns
read_buffer_size=8192
max_buffer_size=819200


[[graphite-proxy.servers]]
servers=["tcp://127.0.0.1:2013"]

[graphite-gg-relay]
listen="backend_only"
msg_type="graphite"

# graphite style
hasher_algo="md5"
hasher_elter="graphite"
hasher_vnodes=1

## need higher timeouts for graphite
write_timeout=5000
runner_timeout=5000
pool_buffersize=16384

#TCP buffers should be smaller as there are Many TCP conns
read_buffer_size=8192
max_buffer_size=819200

[[graphite-gg-relay.servers]]
servers=["tcp://127.0.0.1:2013"]
