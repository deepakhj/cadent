[default]

## PID file to check if things is running already
pid_file="/tmp/consthash.pid"

## user this many proc while active (will default to the number on the machine)
# 0 == max CPUs
num_procs=0


# spit out some stats one 5sec intervals per server
stats_tick=true

## if you DO NOT want to pool connections (might be better for certain situations)
## `single` -> One connection = one line
## `pool` -> Pools connections to each server uses `max_pool_connections` for the pooling amount
## `bufferedpool` -> Like pool, except buffer outgoing writes to save Lots on I/O usage,
##   your receiver needs to be able to handel multiline inputs,
##   this is the default method, lines flush automatically every 1 second
##   even if the buffer is not full
sending_method="bufferedpool"

## timeouts when sending lines to the receiver (in milliseconds)
write_timeout=1000

## outgoing connections are pooled per outgoing server (above)
## this sets the number of pool connections
## careful with this number, there will be this number * servers sending to connections
## (i.e. file descriptors limits may hit if this is too big)
max_pool_connections=10

## if using the `bufferedpool` this is the size in bytes to use for the buffer
## (defaults to 512)
pool_buffersize=512

## Hashing Algo
# The hashing algo to use in our hashring
# 'crc32' -> default
# 'md5' -> the first 4 bytes of the MD5 (same one graphite uses)
# 'sha1' -> the first 4 bytes of the SHA1
# 'fnv' -> 32 bit FNV
# 'fnva' -> 32 bin FNVa -- a faster fnv
# 'nodejs-hashring' -> an "odd" bitshifting md5 one used by Statsd (nodejs-hashring really)
# 'mmh3' -> MurmurHash fast hasher .. the one cassandra uses by default
hasher_algo="md5"

## Hashing Etls
# The hashing elt that composes the acctual hashing server key
# 'graphite' -> the one graphite uses (default)
# 'statsd' -> the one statsd uses
# 'goconsistent' -> the one uses in the https://github.com/stathat/consistent
hasher_elter="graphite"

## Hasher vnodes
# the number of replicas per server
## graphite has 100 for this
## statsd has 40
## NOTE: this is NOT for "replication of data" but for how many virtual nodes are
## in the ConsistentHash ring per "real" node .. sometimes called "vnodes"
hasher_vnodes=40

## Data Replication
### for each line we get we can replicated it other nodes in the server
### list. (1 s the default, just send to one server)
num_dupe_replicas=1

## cache keys/server pairs in an LRU mem cache for speed
cache_items=50000

## Health Checking options
### check every "X" seconds
heartbeat_time_delay=60

### Timeout for failed connections (in Seconds)
heartbeat_time_timeout=1

## if a server has failed "X" times it's out of the loop
failed_heartbeat_count=3

## If we detect a downed node,
## `remove_node` -- we can REMOVE the node from the hashpool
## `ignore` -- stop checking the node
server_down_policy="remove_node"

## Turn on CPU/Mem profiling
##  there will be a http server set up to yank pprof data on :6060
cpu_profile=true

## fire out some internal to statsd if desired
statsd_server="192.168.59.103:8125"
statsd_prefix="consthash"
statsd_interval=1  # send to statd every second (buffered)

# global statsd Sample Rates
## It's HIGHLY recommended you at least put a statsd_timer_sample_rate as we measure
## rates of very fast functions, and it can take _alot_ of CPU cycles just to do that
## unless your server not under much stress 1% is a good number
## `statsd_sample_rate` is for counters
## `statsd_timer_sample_rate` is for timers
statsd_timer_sample_rate=0.01
statsd_sample_rate=0.1


## number of workers to chew on the sending queue
## adjust this based on the input load
workers=200

## timeouts for items on the worker queue in millisconds
runner_timeout=1000

## there will be an internal health http server set up as well
## and will respond to '/ops/status' -> "GET OK"
## '/stats' --> blob of json data (adding ?jsonp=xxx will return jsonp data)
## '/' --> a little file that charts (poorly) the above stats
internal_health_server_listen="0.0.0.0:6061"

## stats are "rendered" every 5 seconds, so this will keep last
## `internal_health_server_points` in RAM (careful with how many you store)
## (use the statsd emitter to store longer term stuff)
internal_health_server_points=500


## path to the html files for the internal health serer
internal_health_server_path="html"

## read buffers for incomming connections
# UPD need a larger read buffer as there is only "one" client
# read_buffer_size=1048576
# max_read_buffer_size=10485760

# TCP connections need a smaller read buffer as there can be MANY connections
read_buffer_size=8192
max_read_buffer_size=819200


### Server sections .. note all the above is overridable in each section
### (except the `statsd_*` and `internal_health_server_listen`)
### NOTE: `hasher_replicas` will NOT use the defaults, be explict there

# ==================== STATSD ====================

[statsd-example]

## what port/scheme we are listening on for incoming data

listen="udp://127.0.0.1:8125"

## msg_type = statsd | graphite | regex
## <key>:<stat>
msg_type="statsd"

# statsd style
hasher_algo="nodejs-hashring"
hasher_elter="statsd"
hasher_vnodes=40

# UPD need a larger read buffer as there is only "one" client
# read_buffer_size=1048576
# max_read_buffer_size=10485760

##
## Yes this can be MULTIPLE server groups
## so you can replicate data across farms
##

[[statsd-example.servers]]
## the list of servers we hashring send to
## {scheme}://{ip}:{port},...
## scheme = tcp|udp
servers=["udp://192.168.59.103:8126", "udp://192.168.59.103:8136", "udp://192.168.59.103:8146"]

## there must be a check server for each `servers` above
## {scheme}://{ip}:{port}:{hashkey},...
## scheme = tcp|http -- NOTE NO UDP checks (does not make sense)
## tcp connections are simply checked for a connection and closed
## http connections must respond with a 200 to a GET on the provided url
## the `{hashkey}` will default to `{scheme}://{ip}:{port}` if none is provided
## (this syntax is made to match hash rings key systems for systems you need to replace by this one)
check_servers=["tcp://192.168.59.103:8127", "tcp://192.168.59.103:8137", "tcp://192.168.59.103:8147"]

##
## `hashkeys`
## these are the actual hash keys used in the hashring placement for each server
## they default to the servers themselves w/o the "udp://, tcp://" attached
## any distinct string will do here
## the below is the default for the servers above, this is just explicit
## hashkeys=["192.168.59.103:8126", "192.168.59.103:8136", "192.168.59.103:8146"]
#

# ---------- replica set ---------------
# replica
# [[statsd-example.servers]]
# servers=...
# check_servers=....
# haskeys=...


# ==================== GRAPHITE ====================

[graphite-example]

## msg_type = statsd | graphite | regex
## <time> <key> <value>

msg_type="graphite"

# graphite style
hasher_algo="md5"
hasher_elter="graphite"
hasher_vnodes=100

# TCP connections need a smaller read buffer as there can be MANY connections
read_buffer_size=8192
max_read_buffer_size=819200


[[graphite-example.servers]]

servers=["tcp://192.168.10.200:2003","tcp://192.168.10.201:2003","tcp://192.168.10.202:2003"]

#check_servers will default to the servers

##
## NOTE about graphite's hasher .. typically you have the carbon-relay's destinations server set up like
## `DESTINATIONS=192.168.10.200:2004:a,192.168.10.201:2004:b,192.168.10.202:2004:c`
## The hash keys it sends to it's internal HashRing are actually "tuples" put into a string the the port removed
## so the hash key for things looks like `('192.168.10.200', 'a')`
## so if you need to match graphite's exact hash config, the hashkeys need to be (based on the servers above)
## you'll need to look up the "a/b/c substring on the
##

hashkeys=["""('192.168.10.200', 'a')""","""('192.168.10.201', 'b')""","""('192.168.10.202', 'c')"""]

# ---------- replica set ---------------

# replica
# [[graphite-example.servers]]
# servers=...

# ==================== REGEX ====================

[regex-example]

# syslog like to farm specific hosts
listen="udp://127.0.0.1:6004"

## msg_type = statsd | graphite | regex
## NOTE  msg_regex needs to have (?P<Key>..) named bit
msg_type="regex"

## NOTE: SINGLE QUOTES ARE IMPORTANT otherwise it will try to unescape the string

msg_regex='^(<\d+>)?(?P<Timestamp>[A-Z][a-z]+\s+\d+\s\d+:\d+:\d+) (?P<Key>\S+) (?P<Logger>\S+):(.*)'
hasher_algo="fnv"

[[regex-example.servers]]
servers=["udp://127.0.0.1:6080","udp://127.0.0.1:6081","udp://127.0.0.1:6082"]
check_servers=["tcp://127.0.0.1:6090","tcp://127.0.0.1:6091","tcp://127.0.0.1:6092"]


# ==================== BackendOnly ====================

## Backends only
## this is a outgoing only set where by there is no front facing socket to field requests
## this is to be used for PreReg entities that prefilter any incoming and can be refered
## to by the name `graphite-aux`
## (see example-prereg.toml)
[graphite-aux]

## backend only
listen="backend_only"

msg_type="graphite"

# graphite style
hasher_algo="md5"
hasher_elter="graphite"
hasher_vnodes=100


[[graphite-aux.servers]]
servers=["tcp://192.168.12.200:2003","tcp://192.168.12.201:2003","tcp://192.168.12.202:2003"]

